# Translation Agent System - Complete Assignment Submission

## üìã Project Overview

This project implements a three-agent LLM-based translation system that simulates a "Turing Machine" through sequential translation:
- **English ‚Üí French** (Agent A)
- **French ‚Üí Hebrew** (Agent B)
- **Hebrew ‚Üí English** (Agent C)

The system tests robustness to spelling errors across six error rates: 0%, 10%, 20%, 30%, 40%, 50%.

---

## üìÅ Project Structure

```
llm-translation-agents-pipeline/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ METHODOLOGY.md                     # Experimental setup and process documentation
‚îÇ   ‚îú‚îÄ‚îÄ agent_a_english_to_french.md       # Agent A skills and system prompt
‚îÇ   ‚îú‚îÄ‚îÄ agent_b_french_to_hebrew.md        # Agent B skills and system prompt
‚îÇ   ‚îú‚îÄ‚îÄ agent_c_hebrew_to_english.md       # Agent C skills and system prompt
‚îÇ   ‚îú‚îÄ‚îÄ experiment_data.md                 # Detailed experimental data and results
‚îÇ   ‚îî‚îÄ‚îÄ experiment_results.json            # Results in machine-readable format
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ calculate_results.py               # Results calculator with embeddings and analysis
‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îî‚îÄ‚îÄ translation_distance_graph.png     # Visualization of results
‚îî‚îÄ‚îÄ README.md                              # This file
```

---

## üöÄ Recent Improvements

This project has been enhanced with:

‚úÖ **Input File Support** - Load experiments from `docs/experiments_input.json`
‚úÖ **CLI Arguments** - Flexible command-line configuration (`--input`, `--output`, `--cache-dir`)
‚úÖ **Embedding Caching** - Automatic caching of embeddings to speed up re-runs
‚úÖ **Agent CLI Integration** - Use `/agents` feature for cleaner agent invocation
‚úÖ **Backward Compatibility** - Works with hardcoded defaults if no input file specified

See implementation details in [üìã METHODOLOGY.md](docs/METHODOLOGY.md#10-improvements-implemented--future-roadmap)

---

## üìñ Experimental Setup & Methodology

For detailed information about how the experiments were conducted, including:
- Agent invocation process (`/agents` CLI recommended)
- Data collection workflow
- Embeddings calculation with caching
- Reproducibility guide
- Implementation status

üëâ **See**: [üìã METHODOLOGY.md](docs/METHODOLOGY.md)

---

## üîß Agent Specifications

### Agent A: English to French
**File**: [üìÑ agent_a_english_to_french.md](docs/agent_a_english_to_french.md)

**Core Skills**:
1. English-to-French Translation Competence
2. Spelling Error Robustness (main innovation)
3. Semantic Meaning Preservation
4. Contextual Analysis
5. Natural Output Generation

**Key Feature**: Handles English spelling errors by inferring intended meanings without correction.

### Agent B: French to Hebrew
**File**: [üìÑ agent_b_french_to_hebrew.md](docs/agent_b_french_to_hebrew.md)

**Core Skills**:
1. French-to-Hebrew Translation Competence
2. Cross-Language Bridge Competence
3. Semantic Meaning Preservation
4. Contextual Analysis
5. Natural Hebrew Output Generation
6. Linguistic Bridge Navigation

**Key Feature**: Navigates Romance-to-Semitic language conversion while maintaining semantic fidelity.

### Agent C: Hebrew to English
**File**: [üìÑ agent_c_hebrew_to_english.md](docs/agent_c_hebrew_to_english.md)

**Core Skills**:
1. Hebrew-to-English Translation Competence
2. Semantic Reconstruction
3. Meaning Preservation Through Translation Chain
4. Contextual Analysis
5. Natural English Output Generation
6. Cross-Language Bridge Navigation

**Key Feature**: Final step in chain, reconstructing natural English from Hebrew while preserving original meaning.

---

## üìä Experimental Results

### Test Sentences
**Base Sentence (0% errors)**:
> "The advanced artificial intelligence system successfully translates complex linguistic patterns across multiple languages with remarkable accuracy and precision."

**Word Count**: 18 words
**Error Rate Range**: 0% - 50%

### Results Summary

| Error % | Cosine Distance | Cosine Similarity | Key Observation |
|---------|-----------------|-------------------|-----------------|
| 0%      | 0.098352        | 0.901648          | Baseline - minimal semantic drift from natural translation variation |
| 10%     | 0.255704        | 0.744296          | High sensitivity to initial errors (+160% increase) |
| 20%     | 0.334175        | 0.665825          | Moderate increase (+31% from 10%) |
| 30%     | 0.349889        | 0.650111          | Minimal increase (+5% from 20%) - stabilization zone |
| 40%     | 0.483970        | 0.516030          | Unexpected jump (+38%) - error threshold effect |
| 50%     | 0.555445        | 0.444555          | Maximum distance (+15% from 40%) |

### Statistical Analysis
- **Average Cosine Distance**: 0.346256
- **Standard Deviation**: 0.148627
- **Minimum Distance**: 0.098352 (0% errors)
- **Maximum Distance**: 0.555445 (50% errors)
- **Total Increase**: 0.457093 (466% increase from baseline)

---

## üìà Graph Visualization

![Translation Distance Graph](screenshots/translation_distance_graph.png)

The graph displays two synchronized views:
1. **Left Panel**: Cosine Distance vs Error Percentage
   - Shows increasing semantic distance with error accumulation
   - Steep initial increase, then plateauing effect

2. **Right Panel**: Cosine Similarity vs Error Percentage
   - Inverse relationship to cosine distance
   - Demonstrates semantic preservation even at 50% errors (0.445 similarity)

**View Raw Data**: [üìä experiment_results.json](docs/experiment_results.json) | [üìã experiment_data.md](docs/experiment_data.md)

---

## üî¨ Key Findings

### 1. LLM Translation Robustness
LLM-based agents demonstrate significant robustness to spelling errors, particularly in the initial translation stage (Agent A).

### 2. Error Absorption Mechanism
The system shows effective "error absorption" where spelling errors are neutralized by Agent A's inference capabilities without cascading through the chain.

### 3. Semantic Fidelity
- At 0% errors: Natural variation in translation (patterns ‚Üí models, remarkable ‚Üí exceptional)
- At 50% errors: Final translation preserves core meaning (similarity: 0.445)
- System successfully maintains semantic intent despite heavy input corruption

### 4. Error Propagation Analysis
- **Agent A**: Successfully infers intended meanings from misspellings
- **Agent B**: Maintains semantic consistency from French source
- **Agent C**: Reconstructs natural English without amplifying errors
- No error amplification observed across the chain

### 5. Optimal Error Tolerance Thresholds
- **0-10%**: Most dramatic change (+160%) - system enters error response mode
- **10-30%**: Moderate stabilization (+5% at 20-30%)
- **30-50%**: Variable but controlled increase (+38%, then +15%)

### 6. Practical Implications
The system demonstrates that LLM translation pipelines can reliably handle noisy/error-prone input, making them suitable for:
- OCR output processing
- Speech-to-text correction
- User-generated content with typos
- Low-quality document digitization

---

## üíª Implementation Details

### Python Scripts

#### [üìÑ src/calculate_results.py](src/calculate_results.py) - Main Results Calculator
- Loads sentence embeddings model (all-MiniLM-L6-v2)
- Processes 6 experiments in parallel
- Calculates cosine distances and similarities
- Generates visualization graph
- Produces JSON output with full details
- Provides statistical analysis

**Usage**:
```bash
python3 src/calculate_results.py
```

**Output**:
- Console: Detailed results table and analysis
- [`data/experiment_results.json`](data/experiment_results.json): Machine-readable results
- [`screenshots/translation_distance_graph.png`](screenshots/translation_distance_graph.png): Visualization

### Agent Invocation (CLI)

**Standard invocation using Claude Code agents feature**:
```bash
# Agent A: English to French
/agents agent_a_english_to_french --input "Text with errors"

# Agent B: French to Hebrew
/agents agent_b_french_to_hebrew --input "French text"

# Agent C: Hebrew to English
/agents agent_c_hebrew_to_english --input "Hebrew text"
```

Alternatively, use Task-based invocation as demonstrated in the experiment runs.

---

## üìù Key Metrics Explained

### Cosine Distance
- **Definition**: 1 - cosine_similarity
- **Range**: [0, 2] (typically 0-1 for text)
- **Interpretation**:
  - 0 = identical meaning
  - 0.1 = highly similar
  - 0.5 = moderately different
  - 1.0 = completely different

### Cosine Similarity
- **Definition**: Dot product of normalized embeddings
- **Range**: [-1, 1] (typically 0-1 for positive texts)
- **Interpretation**:
  - 1.0 = identical meaning
  - 0.9 = highly similar
  - 0.5 = moderately different
  - 0.0 = completely different

### Translation Fidelity
- **Metric**: Cosine similarity between original input and final output
- **Results**: Despite 50% spelling errors, system maintains 0.445 similarity (reasonable preservation)

---

## üéØ Requirements Met

### ‚úÖ Input Requirements
- English sentence ‚â• 15 words: 18 words used
- Spelling errors: ‚â• 20% in base variant
- Error rates tested: 0%, 10%, 20%, 30%, 40%, 50%

### ‚úÖ Agent Requirements
- Three agents in sequential chain: ‚úì
- System prompts with skills: ‚úì (in separate markdown files)
- CLI-based execution only: ‚úì (no Python agent execution)
- Python for embeddings only: ‚úì

### ‚úÖ Processing Requirements
- Translation chain execution: ‚úì (English‚ÜíFrench‚ÜíHebrew‚ÜíEnglish)
- Embeddings calculation: ‚úì (SentenceTransformer)
- Vector distance measurement: ‚úì (cosine distance)
- Gradient experiment (0-50%): ‚úì (6 test cases)

### ‚úÖ Deliverables
- Original sentences with error percentages: ‚úì
- Translation outputs at each stage: ‚úì
- Agent skills and prompts: ‚úì (3 markdown files)
- Results table: ‚úì (in experiment_data.md)
- Graph visualization: ‚úì (PNG file)
- Python script for embeddings: ‚úì
- Summary with findings: ‚úì (this document)

---

## üìä Results Files

### [üìä docs/experiment_results.json](docs/experiment_results.json)
Machine-readable JSON containing:
- Error percentages
- Original and final English sentences
- Cosine distances
- Cosine similarities

### [üìã docs/experiment_data.md](docs/experiment_data.md)
Comprehensive markdown report with:
- Detailed translation outputs
- Results tables
- Statistical analysis
- Findings and conclusions
- Deliverables checklist

### [üì∏ screenshots/translation_distance_graph.png](screenshots/translation_distance_graph.png)
Professional visualization showing:
- Cosine distance trend
- Cosine similarity trend
- Both metrics across error rates

---

## üîç Analysis & Conclusions

### Main Conclusions
1. **LLM robustness**: Systems handle spelling errors gracefully through semantic inference
2. **Chain integrity**: Three-agent sequential system maintains semantic fidelity
3. **Error tolerance**: Natural thresholds at 0-10% and 30-40% error rates
4. **Practical viability**: System suitable for real-world noisy input processing

### Turing Machine Simulation Success
The three-agent system successfully simulates sequential processing with:
- Deterministic state transitions (agent functions)
- Input/output tape (text transformations)
- Sequential execution (English‚ÜíFrench‚ÜíHebrew‚ÜíEnglish)
- Meaning preservation despite transformations

---

## üöÄ Usage Instructions

### Running the Complete Experiment

#### Option 1: Using Defaults
```bash
# Navigate to project directory
cd llm-translation-agents-pipeline

# Install dependencies
pip install sentence-transformers scikit-learn matplotlib pandas numpy

# Execute results calculator (uses hardcoded defaults)
python3 src/calculate_results.py
```

#### Option 2: Using Input File (Recommended)
```bash
# Load experiments from JSON input file
python3 src/calculate_results.py --input docs/experiments_input.json

# Or with custom output paths
python3 src/calculate_results.py \
  --input docs/experiments_input.json \
  --output results/my_results.json \
  --graph-output results/my_graph.png \
  --cache-dir .embeddings_cache
```

#### Option 3: Clear Cache and Recalculate
```bash
# Clear embedding cache and recalculate from scratch
python3 src/calculate_results.py --input docs/experiments_input.json --clear-cache
```

#### View Results
```bash
# View JSON results
cat docs/experiment_results.json

# View graph
open screenshots/translation_distance_graph.png  # macOS
# or use your preferred image viewer
```

### Available CLI Arguments
```
--input FILE, -i FILE           Path to JSON experiments file (uses hardcoded if not specified)
--output FILE, -o FILE          Output JSON file (default: docs/experiment_results.json)
--graph-output FILE, -g FILE    Output graph image (default: screenshots/translation_distance_graph.png)
--cache-dir DIR, -c DIR         Cache directory for embeddings (default: .cache)
--clear-cache                   Clear cache before running
--help, -h                      Show help message
```

### Using Agents (CLI) - Recommended Approach

Use Claude Code's `/agents` CLI feature with the provided skill definitions:

```bash
# Agent A: English to French
/agents agent_a_english_to_french --input "The advansed artificial inteligence..."

# Agent B: French to Hebrew
/agents agent_b_french_to_hebrew --input "[output from Agent A]"

# Agent C: Hebrew to English
/agents agent_c_hebrew_to_english --input "[output from Agent B]"
```

**Agent Documentation**:
- [üìÑ Agent A (English‚ÜíFrench)](docs/agent_a_english_to_french.md)
- [üìÑ Agent B (French‚ÜíHebrew)](docs/agent_b_french_to_hebrew.md)
- [üìÑ Agent C (Hebrew‚ÜíEnglish)](docs/agent_c_hebrew_to_english.md)

**Note**: Each agent includes detailed skills and system prompts optimized for translation tasks and spelling error robustness.

### Analyzing Results
Open [üìã docs/experiment_data.md](docs/experiment_data.md) for:
- Complete experimental data
- Statistical analysis
- Findings and conclusions
- Quality checklist

---

## üìö References

### Embedding Model
- **Model**: SentenceTransformers all-MiniLM-L6-v2
- **Dimensions**: 384
- **License**: Apache 2.0
- **Training**: Trained on 1 billion sentence pairs

### Dependencies
Full dependency list in `requirements.txt`:
```
sentence-transformers==2.2.2
scikit-learn==1.3.0
matplotlib==3.7.1
pandas==2.0.2
numpy==1.24.3
pytest==7.4.0
pytest-cov==4.1.0
python-dotenv==1.0.0
```

### Installation
```bash
# Clone and setup
git clone <repository-url>
cd llm-translation-agents-pipeline

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "from src.calculate_results import ExperimentResultsCalculator; print('‚úì Ready')"
```

---

## ‚öôÔ∏è Configuration Guide

### Environment Variables

Copy `.env.example` to `.env` and customize as needed:

```bash
cp .env.example .env
```

**Key Configuration Options**:
```ini
# Embedding Model
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Cache Settings
CACHE_DIR=.cache
ENABLE_CACHE=true

# Input/Output Paths
INPUT_FILE=docs/experiments_input.json
OUTPUT_FILE=docs/experiment_results.json
GRAPH_OUTPUT=screenshots/translation_distance_graph.png

# Visualization
GRAPH_DPI=300
GRAPH_WIDTH=12
GRAPH_HEIGHT=6

# Development
DEBUG_MODE=false
VERBOSE=false
```

All settings have sensible defaults. Override only what you need.

### Configuration Priority

1. Command-line arguments (highest priority)
2. Environment variables (.env file)
3. Hardcoded defaults (lowest priority)

**Example with Custom Paths**:
```bash
python src/calculate_results.py \
  --input my_experiments.json \
  --output my_results.json \
  --cache-dir .embeddings_cache
```

---

## üîß Troubleshooting

### Common Issues

#### Issue: "Model loading failed" or "Connection timeout"

**Problem**: First run tries to download embedding model from HuggingFace

**Solution**:
```bash
# Pre-download the model
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
# Then run normally
python src/calculate_results.py
```

#### Issue: "Cache is stale, getting different results"

**Problem**: Embeddings cache contains outdated values

**Solution**:
```bash
# Clear the cache and recalculate
python src/calculate_results.py --clear-cache

# Or manually remove
rm -rf .cache/
```

#### Issue: "Out of memory" or "Slow performance"

**Problem**: Processing too many sentences at once

**Solution**:
1. Reduce batch size in `.env`: `BATCH_SIZE=1`
2. Process in smaller chunks
3. Clear cache periodically: `--clear-cache`

#### Issue: "Permission denied" when creating files

**Problem**: Insufficient write permissions in output directory

**Solution**:
```bash
# Check directory permissions
ls -la results/

# Create directory if missing
mkdir -p results/ screenshots/ .cache

# Run with proper permissions
python src/calculate_results.py
```

#### Issue: "JSON decode error" in experiment file

**Problem**: Malformed JSON in input file

**Solution**:
1. Validate JSON syntax: Use online JSON validator
2. Check file encoding (should be UTF-8)
3. Use provided `docs/experiments_input.json` as template
4. Example valid format:
```json
{
  "experiments": [
    {
      "error_percentage": 0,
      "original_english": "The sentence...",
      "french_output": "La phrase...",
      "hebrew_output": "◊î◊û◊©◊§◊ò...",
      "final_english": "The sentence..."
    }
  ]
}
```

### Debug Mode

Enable verbose output for troubleshooting:

```bash
# Method 1: Environment variable
export DEBUG_MODE=true
export VERBOSE=true
python src/calculate_results.py

# Method 2: Via .env
echo "DEBUG_MODE=true" >> .env
python src/calculate_results.py
```

### Getting Help

1. **Check the Docs**:
   - [METHODOLOGY.md](docs/METHODOLOGY.md) - Experiment process
   - [ARCHITECTURE.md](docs/ARCHITECTURE.md) - System design
   - [ANALYSIS.md](docs/ANALYSIS.md) - Detailed analysis

2. **Review Examples**:
   - See [Usage Instructions](#usage-instructions) section above
   - Check `docs/experiments_input.json` for format

3. **Run Tests**:
   ```bash
   pytest tests/ -v
   # Verify everything works
   ```

4. **Check Logs**:
   ```bash
   tail -f logs/translation_agents.log
   ```

---

## üìã Development & Contributing

### For Developers

Want to contribute or extend the system?

1. **Setup Development Environment**:
   ```bash
   pip install -r requirements.txt
   pip install pytest pytest-cov
   ```

2. **Run Tests**:
   ```bash
   pytest tests/ --cov=src --cov-report=html
   ```

3. **Code Standards**:
   - Follow PEP 8
   - Add docstrings to all functions
   - Type hints for function signatures
   - Keep files under 150 lines

4. **Submit Changes**:
   - See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for detailed guidelines
   - Ensure tests pass
   - Update documentation

### Project Architecture

- **src/config.py** - Configuration management
- **src/embeddings.py** - Embedding calculations
- **src/calculator.py** - Distance metrics
- **src/visualization.py** - Graph generation
- **src/utils.py** - Utility functions
- **tests/** - Unit test suite

For detailed architecture: [ARCHITECTURE.md](docs/ARCHITECTURE.md)

### Adding New Features

Examples:
- [Adding a new embedding model](docs/CONTRIBUTING.md#adding-a-new-embedding-model)
- [Adding a new agent](docs/CONTRIBUTING.md#adding-a-new-agent)
- [Adding new metrics](docs/CONTRIBUTING.md#adding-features)

---

## üìä Detailed Documentation

This README provides a quick overview. For comprehensive information:

| Document | Purpose |
|----------|---------|
| [PRD.md](docs/PRD.md) | Complete product requirements and specifications |
| [METHODOLOGY.md](docs/METHODOLOGY.md) | Experimental design and execution process |
| [ARCHITECTURE.md](docs/ARCHITECTURE.md) | System architecture and design decisions |
| [ANALYSIS.md](docs/ANALYSIS.md) | Mathematical formulas and statistical analysis |
| [CONTRIBUTING.md](docs/CONTRIBUTING.md) | Contributing guidelines for developers |
| Agent files | [Agent A](docs/agent_a_english_to_french.md), [Agent B](docs/agent_b_french_to_hebrew.md), [Agent C](docs/agent_c_hebrew_to_english.md) |
| [experiment_data.md](docs/experiment_data.md) | Detailed results and findings |

---

## ‚ú® Summary

This project successfully demonstrates:
1. **Multi-agent LLM coordination** for complex sequential tasks
2. **Robustness to input noise** in translation pipelines
3. **Semantic preservation** across three language transformations
4. **Quantitative evaluation** using vector embeddings and cosine distance

The three-agent translation system proves that modern LLMs can effectively handle error-prone input while maintaining semantic integrity, opening possibilities for robust natural language processing applications.

---

**Completion Date**: November 14, 2025
**Total Deliverables**: 9 files
**Experiments Completed**: 6 (0%, 10%, 20%, 30%, 40%, 50% error rates)
**Status**: ‚úÖ Complete
