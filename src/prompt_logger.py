#!/usr/bin/env python3
"""
Prompt Logger - Record all prompts used with complete metadata

This module provides comprehensive logging of all LLM agent prompts, including
input, output, model parameters, and execution metadata for reproducibility
and analysis.

Usage:
    from prompt_logger import PromptLogger

    logger = PromptLogger("results/prompt_execution_log.json")
    logger.log_prompt(
        agent="Agent_A",
        version="2.0",
        model="gpt-4-turbo",
        input_text="The quick brown fox...",
        output_text="Le renard brun rapide...",
        metadata={"error_rate": 0, "temperature": 0.3}
    )
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import hashlib
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PromptLogger:
    """
    Comprehensive prompt logging system for LLM agent execution.

    Features:
    - Records all prompt executions with full metadata
    - Tracks prompt versions and evolution
    - Stores input/output pairs for analysis
    - Generates unique IDs for each execution
    - Supports querying and filtering
    - Thread-safe JSON file operations
    """

    def __init__(self, log_file: str = "results/prompt_execution_log.json"):
        """
        Initialize prompt logger.

        Args:
            log_file: Path to JSON log file
        """
        self.log_file = Path(log_file)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # Initialize log file if it doesn't exist
        if not self.log_file.exists():
            self._initialize_log_file()

        logger.info(f"PromptLogger initialized: {self.log_file}")

    def _initialize_log_file(self):
        """Initialize empty log file with metadata."""
        initial_data = {
            "metadata": {
                "created": datetime.now().isoformat(),
                "version": "1.0",
                "description": "LLM Translation Agents Prompt Execution Log",
                "project": "Multi-Agent Translation Chain"
            },
            "prompts": []
        }

        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, ensure_ascii=False, indent=2)

        logger.info("Initialized new log file")

    def _load_log(self) -> Dict:
        """Load current log data."""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_log(self, data: Dict):
        """Save log data."""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _generate_execution_id(self, agent: str, timestamp: str, input_text: str) -> str:
        """
        Generate unique execution ID.

        Args:
            agent: Agent name
            timestamp: ISO timestamp
            input_text: Input text

        Returns:
            Unique hex ID
        """
        content = f"{agent}_{timestamp}_{input_text}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def log_prompt(
        self,
        agent: str,
        version: str,
        model: str,
        system_prompt: str,
        input_text: str,
        output_text: str,
        temperature: float = 0.3,
        max_tokens: int = 2000,
        error_rate: Optional[float] = None,
        tokens_used: Optional[Dict[str, int]] = None,
        execution_time: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Log a prompt execution with complete metadata.

        Args:
            agent: Agent identifier (e.g., "Agent_A", "Agent_B", "Agent_C")
            version: Prompt version (e.g., "2.0")
            model: Model identifier (e.g., "gpt-4-turbo")
            system_prompt: Full system prompt used
            input_text: Input provided to agent
            output_text: Output generated by agent
            temperature: Temperature parameter
            max_tokens: Max tokens parameter
            error_rate: Error rate of input (0-50)
            tokens_used: Dict with prompt_tokens, completion_tokens, total_tokens
            execution_time: Execution time in seconds
            metadata: Additional metadata

        Returns:
            Unique execution ID
        """
        timestamp = datetime.now().isoformat()
        execution_id = self._generate_execution_id(agent, timestamp, input_text)

        # Build execution record
        execution_record = {
            "execution_id": execution_id,
            "timestamp": timestamp,
            "agent": agent,
            "prompt_version": version,
            "model": {
                "name": model,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            "system_prompt": system_prompt,
            "input": {
                "text": input_text,
                "length": len(input_text),
                "word_count": len(input_text.split()),
                "error_rate": error_rate
            },
            "output": {
                "text": output_text,
                "length": len(output_text),
                "word_count": len(output_text.split())
            },
            "tokens": tokens_used or {
                "prompt_tokens": None,
                "completion_tokens": None,
                "total_tokens": None
            },
            "execution_time_seconds": execution_time,
            "metadata": metadata or {}
        }

        # Load, append, save
        data = self._load_log()
        data["prompts"].append(execution_record)
        self._save_log(data)

        logger.info(f"Logged prompt execution: {agent} [{execution_id}]")
        return execution_id

    def get_executions_by_agent(self, agent: str) -> List[Dict]:
        """
        Get all executions for a specific agent.

        Args:
            agent: Agent identifier

        Returns:
            List of execution records
        """
        data = self._load_log()
        return [p for p in data["prompts"] if p["agent"] == agent]

    def get_executions_by_error_rate(self, error_rate: float) -> List[Dict]:
        """
        Get all executions for a specific error rate.

        Args:
            error_rate: Error rate (0-50)

        Returns:
            List of execution records
        """
        data = self._load_log()
        return [
            p for p in data["prompts"]
            if p["input"].get("error_rate") == error_rate
        ]

    def get_execution_by_id(self, execution_id: str) -> Optional[Dict]:
        """
        Get specific execution by ID.

        Args:
            execution_id: Execution ID

        Returns:
            Execution record or None
        """
        data = self._load_log()
        for prompt in data["prompts"]:
            if prompt["execution_id"] == execution_id:
                return prompt
        return None

    def get_all_executions(self) -> List[Dict]:
        """
        Get all execution records.

        Returns:
            List of all execution records
        """
        data = self._load_log()
        return data["prompts"]

    def get_statistics(self) -> Dict:
        """
        Get statistics about logged executions.

        Returns:
            Dictionary with statistics
        """
        data = self._load_log()
        prompts = data["prompts"]

        if not prompts:
            return {"total_executions": 0}

        # Calculate statistics
        agents = {}
        error_rates = {}
        models = {}
        total_tokens = 0
        total_time = 0

        for p in prompts:
            # Count by agent
            agent = p["agent"]
            agents[agent] = agents.get(agent, 0) + 1

            # Count by error rate
            error_rate = p["input"].get("error_rate")
            if error_rate is not None:
                error_rates[error_rate] = error_rates.get(error_rate, 0) + 1

            # Count by model
            model = p["model"]["name"]
            models[model] = models.get(model, 0) + 1

            # Sum tokens
            if p["tokens"]["total_tokens"]:
                total_tokens += p["tokens"]["total_tokens"]

            # Sum time
            if p["execution_time_seconds"]:
                total_time += p["execution_time_seconds"]

        return {
            "total_executions": len(prompts),
            "executions_by_agent": agents,
            "executions_by_error_rate": error_rates,
            "executions_by_model": models,
            "total_tokens_used": total_tokens,
            "total_execution_time_seconds": round(total_time, 2),
            "average_execution_time_seconds": round(total_time / len(prompts), 2) if prompts else 0,
            "first_execution": prompts[0]["timestamp"],
            "last_execution": prompts[-1]["timestamp"]
        }

    def export_to_csv(self, output_file: str):
        """
        Export log to CSV format.

        Args:
            output_file: Path to output CSV file
        """
        import pandas as pd

        data = self._load_log()
        prompts = data["prompts"]

        # Flatten records for CSV
        flattened = []
        for p in prompts:
            flattened.append({
                "execution_id": p["execution_id"],
                "timestamp": p["timestamp"],
                "agent": p["agent"],
                "prompt_version": p["prompt_version"],
                "model": p["model"]["name"],
                "temperature": p["model"]["temperature"],
                "error_rate": p["input"].get("error_rate"),
                "input_length": p["input"]["length"],
                "input_word_count": p["input"]["word_count"],
                "output_length": p["output"]["length"],
                "output_word_count": p["output"]["word_count"],
                "total_tokens": p["tokens"]["total_tokens"],
                "execution_time": p["execution_time_seconds"]
            })

        df = pd.DataFrame(flattened)
        df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"Exported {len(df)} executions to {output_file}")

    def generate_report(self, output_file: str):
        """
        Generate HTML report of executions.

        Args:
            output_file: Path to output HTML file
        """
        stats = self.get_statistics()
        data = self._load_log()

        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Prompt Execution Report</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 40px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2E86AB;
            border-bottom: 3px solid #2E86AB;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #A23B72;
            margin-top: 30px;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background-color: #2E86AB;
            color: white;
        }}
        tr:hover {{
            background-color: #f5f5f5;
        }}
        .stat-box {{
            display: inline-block;
            background-color: #f9f9f9;
            padding: 15px 25px;
            margin: 10px;
            border-left: 4px solid #2E86AB;
        }}
        .stat-value {{
            font-size: 24px;
            font-weight: bold;
            color: #2E86AB;
        }}
        .stat-label {{
            font-size: 14px;
            color: #666;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>Prompt Execution Report</h1>
        <p>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>

        <h2>Summary Statistics</h2>
        <div class="stat-box">
            <div class="stat-value">{stats['total_executions']}</div>
            <div class="stat-label">Total Executions</div>
        </div>
        <div class="stat-box">
            <div class="stat-value">{stats['total_tokens_used']:,}</div>
            <div class="stat-label">Total Tokens</div>
        </div>
        <div class="stat-box">
            <div class="stat-value">{stats['total_execution_time_seconds']:.1f}s</div>
            <div class="stat-label">Total Time</div>
        </div>
        <div class="stat-box">
            <div class="stat-value">{stats['average_execution_time_seconds']:.2f}s</div>
            <div class="stat-label">Avg Time/Execution</div>
        </div>

        <h2>Executions by Agent</h2>
        <table>
            <tr>
                <th>Agent</th>
                <th>Count</th>
            </tr>
        """

        for agent, count in stats['executions_by_agent'].items():
            html += f"<tr><td>{agent}</td><td>{count}</td></tr>"

        html += """
        </table>

        <h2>Executions by Error Rate</h2>
        <table>
            <tr>
                <th>Error Rate (%)</th>
                <th>Count</th>
            </tr>
        """

        for error_rate, count in sorted(stats['executions_by_error_rate'].items()):
            html += f"<tr><td>{error_rate}%</td><td>{count}</td></tr>"

        html += """
        </table>

        <h2>Recent Executions</h2>
        <table>
            <tr>
                <th>Timestamp</th>
                <th>Agent</th>
                <th>Error Rate</th>
                <th>Tokens</th>
                <th>Time (s)</th>
            </tr>
        """

        # Show last 20 executions
        recent = data["prompts"][-20:]
        for p in reversed(recent):
            html += f"""
            <tr>
                <td>{p['timestamp'][:19]}</td>
                <td>{p['agent']}</td>
                <td>{p['input'].get('error_rate', 'N/A')}</td>
                <td>{p['tokens']['total_tokens'] or 'N/A'}</td>
                <td>{p['execution_time_seconds'] or 'N/A'}</td>
            </tr>
            """

        html += """
        </table>
    </div>
</body>
</html>
        """

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)

        logger.info(f"Generated report: {output_file}")


def main():
    """Command-line interface for prompt logger."""
    import argparse

    parser = argparse.ArgumentParser(description='Prompt Logger Utilities')
    parser.add_argument(
        '--log-file',
        default='results/prompt_execution_log.json',
        help='Path to log file'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Show statistics'
    )
    parser.add_argument(
        '--export-csv',
        help='Export to CSV file'
    )
    parser.add_argument(
        '--generate-report',
        help='Generate HTML report'
    )
    parser.add_argument(
        '--agent',
        help='Filter by agent'
    )
    parser.add_argument(
        '--error-rate',
        type=float,
        help='Filter by error rate'
    )

    args = parser.parse_args()

    # Initialize logger
    prompt_logger = PromptLogger(args.log_file)

    # Show statistics
    if args.stats:
        stats = prompt_logger.get_statistics()
        print("\nPrompt Execution Statistics:")
        print("=" * 50)
        print(json.dumps(stats, indent=2))

    # Export to CSV
    if args.export_csv:
        prompt_logger.export_to_csv(args.export_csv)
        print(f"Exported to {args.export_csv}")

    # Generate report
    if args.generate_report:
        prompt_logger.generate_report(args.generate_report)
        print(f"Generated report: {args.generate_report}")

    # Filter by agent
    if args.agent:
        executions = prompt_logger.get_executions_by_agent(args.agent)
        print(f"\nExecutions for {args.agent}: {len(executions)}")
        for exec in executions[-5:]:  # Show last 5
            print(f"  {exec['timestamp']}: {exec['input']['text'][:50]}...")

    # Filter by error rate
    if args.error_rate is not None:
        executions = prompt_logger.get_executions_by_error_rate(args.error_rate)
        print(f"\nExecutions at {args.error_rate}% error rate: {len(executions)}")


if __name__ == '__main__':
    main()
